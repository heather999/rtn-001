\section{Planning and team(s) fro DP0} \label{sec:plan}

Planning epics have been (and are) being created in the \href{https://jira.lsstcorp.org/secure/Dashboard.jspa?selectPageId=15608}{PREOPS Jira} project.
On the dashboard you can see links to the  tickets labeled DP0.1 and DP0.2.

We will have regular (every other week for now) DP0 meetings (see \url{https://confluence.lsstcorp.org/display/LSSTOps/Data+Production+Meetings}).

\subsection {Teams}

The Operations era org chart is shown in \figref{fig:org}.


\begin{figure}
\begin{center}
\includegraphics[width=0.8\textwidth]{images/Ops_Org_Chart}
\end{center}
\caption{ Organization of departments and teams  for operations of Rubin Observatory. \label{fig:org}}
\end{figure}

The main departments involved in DP0 are Data Production and System Performance. With in those departments various people will be involved from the underlying teams but in small numbers. It makes most sense to approach DP0 with a task force approach. This might best be seen as two teams:

\begin{itemize}
\item Data production - with a focus on middleware and execution (\secref{sec:dp});
\item System Performance - with a focus on quality assurance and community support (\secref{sec:sp}).
\end{itemize}

As we advance the teams grow and we will transition to the an organization as in \figref{fig:org}
with team leads for each team as in \figref{fig:dporg}.


\begin{figure}
\begin{center}
\includegraphics[width=0.8\textwidth]{images/DpOrg}
\end{center}
\caption{Data Production team structure \label{fig:dporg}}
\end{figure}

\subsubsection{Task force lead}\label{sec:tfl}
For DP0 on  the IDF  a task force approach seems most appropriate given the partial efforts in all teams.
Hsin-Fang Chiang shall fulfill this role and coordinate Data Production activities  for DP0.
Responsibilities of this role include:
\begin{itemize}
\item Being point of contact for the IDF provider.
\item Setting priorities for all work at the IDF until DP0 is fully complete.
\item Evaluate stage-wise operational readiness wrt. to requirements.
\item Make all components of the IDF work together (Science Platform, Middleware, Workflow ..)
\end{itemize}

The task force lead reports directly to the Data Production Associate Director and
carries delegated authority for the above responsibilities.


\subsection{DP Middleware and Execution}\label{sec:dp}
There is preops effort (fractional FTE) available in Execution and Pipelines as well as Middleware teams.
The roles etc need some clean up from the ops proposal but the DP Roles are listed in \tabref{tab:teamsNames} though the exact mix of roles is still under discussion.


\subsection{SP Quality  and Community Support} \label{sec:sp}

Note: DP0.1 and DP0.2 Early Access described in this document do not leave time for full-scale quality analysis. The provided data will not be science-ready; system performance milestones are succeeding.

{\bf Leanne .. }

\begin{itemize}
\item How do we intend to do support? Slack? JIRA? CLO?
\end{itemize}


\subsection{Planning}


\input{tables/timeline}
Table \ref{tab:timeline} lists internal timeline.

\subsubsection{Middleware}
There are obvious middleware milestones such as L3-MW-0030 read only Gen3 Butler which are needed from the construction project.
There is still installation work needed for the that on Google which includes the need for a Postgress (like) database for the registry. The DAX team are on the hook for this.
For DP0.2 we need Butler to handle processing, not just locating files (L3-MW-0070).

\paragraph{Qserv} should be installed  and configured. Though we have some prior art for this we
still will need some experimentation to get it correct. Getting DC2 loaded in Qserv is also a DAX activity
we will have to do on IDF.

\paragraph{Workflow} needs to be functioning at scale for DP0.2, ideally we should have basic workflow
early on (milestone L3-MW-0050). Then more tooling such as restarting failed jobs (L3-MW-0060).

From the construction side we have BPS as a deliverable which may be useful on IDF also.
We shall evaluate BPS as an option later in 2020 (L3-MW-0040).
See \citeds{LDM-636}, \citeds{LDM-633}, \citeds{DMTN-123}.
BPS translates the quantum graph to DAGMan for execution on HTCondor and submits the jobs.
Most work has gone into the graph and execution.

As part of our march toward a potential more DOE oriented Data Facility, BNL will be part of the pre operations team to experiment with PanDA as an environment to monitor and control our processing jobs.
This is a slightly parallel effort to construction attempting to take advantage of an existing set of tools for large scale job execution.
In an ideal world the quantum graph translation of BPS would feed into a PanDA system to execute (retry etc) our jobs, this is still to be investigated. This may go through CWL.

See also \secref{sec:in2p3}.




\subsubsection{ Science Platform}
The science platform and web services need to be deployed. In principle this is reasonable straight forward, an open issue may be configuring of the Portal aspect for the chosen  dataset(s).

\subsubsection{ Pipelines }
For DP0.2 we need a Gen3 version of the pipelines to process the dataset. This will have to run at scale for PDR2 or DC2. There may be several runs for quality purposes.
Fractional FTE from the Pipelines will provide help in pipeline configuration, data repo preparation, workflow consulting, science verification, data model documenting, troubleshooting, and liaising.
{\bf Yusra will provide more info here.}

\subsubsection{ IN2P3}\label{sec:in2p3}
IN2P3 will contribute in Qserv and pipelines. {\bf Fabio will provide more information here.}
They bring experience running Gen3 workflows. The real interest with IN2P3 is to run remote jobs thus emulating the eventual operational DRP runs. This may be difficult to achieve in FY21 but we should make it a milestone for FY22.\footnote{Tim, Fabio we should set a date for this}. A more achievable goal for FY21 would be to duplicate the IDF processing at IN2P3.

Remote execution requires some  features in Gen3 to be implemented. We will probably wish to execute jobs with a local registry then merge the results and registries.
